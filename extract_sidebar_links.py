#!/usr/bin/env python3
"""
extract_sidebar_links.py

A production-quality CLI tool that extracts left sidebar navigation links
from documentation websites and saves unique URLs to a file.

Usage:
    python extract_sidebar_links.py
    python extract_sidebar_links.py --start-url https://docs.molt.bot/ --out links.txt --verbose

Author: Generated by GitHub Copilot
"""

import argparse
import re
import sys
from typing import Optional
from urllib.parse import urljoin, urlparse, urlunparse

import requests
from bs4 import BeautifulSoup, Tag


# --- Configuration ---
DEFAULT_START_URL = "https://docs.molt.bot/"
DEFAULT_OUTPUT_FILE = "links.txt"
USER_AGENT = "SidebarLinkExtractor/1.0 (CLI tool; +https://github.com/sidebar-extractor)"
REQUEST_TIMEOUT = (10, 30)  # (connect timeout, read timeout) in seconds

# Trailing-slash policy: STRIP trailing slashes for consistency
# Rationale: Most doc sites treat /page and /page/ as equivalent; stripping ensures deduplication.
STRIP_TRAILING_SLASH = True


def normalize_url(url: str, base_url: str, origin: str) -> Optional[str]:
    """
    Normalize a URL for deduplication and filtering.

    Args:
        url: The href value to normalize.
        base_url: The base URL for resolving relative hrefs.
        origin: The origin (scheme + netloc) to filter against.

    Returns:
        Normalized absolute URL if it's internal and valid, None otherwise.

    Normalization rules:
        - Resolve relative hrefs against base_url.
        - Strip fragments (#...).
        - Keep query strings.
        - Normalize scheme to https for the target domain.
        - Apply trailing-slash policy (strip by default).
    """
    if not url:
        return None

    # Skip non-HTTP schemes
    url_lower = url.lower().strip()
    if url_lower.startswith(("mailto:", "tel:", "javascript:", "#")):
        return None

    # Resolve relative URLs
    absolute_url = urljoin(base_url, url)
    parsed = urlparse(absolute_url)

    # Filter: only include same-origin URLs
    target_origin = f"{parsed.scheme}://{parsed.netloc}"
    if target_origin.lower().replace("http://", "https://") != origin.lower():
        return None

    # Normalize scheme to https
    scheme = "https"

    # Strip fragment
    fragment = ""

    # Apply trailing-slash policy to path
    path = parsed.path
    if STRIP_TRAILING_SLASH and path.endswith("/") and len(path) > 1:
        path = path.rstrip("/")

    # Reconstruct normalized URL
    normalized = urlunparse((scheme, parsed.netloc, path, parsed.params, parsed.query, fragment))
    return normalized


def get_origin(url: str) -> str:
    """Extract the origin (scheme + netloc) from a URL, normalized to https."""
    parsed = urlparse(url)
    return f"https://{parsed.netloc}"


def fetch_html(url: str, verbose: bool = False) -> str:
    """
    Fetch HTML content from the given URL.

    Args:
        url: The URL to fetch.
        verbose: If True, print request details.

    Returns:
        The HTML content as a string.

    Raises:
        SystemExit: If the request fails.
    """
    if verbose:
        print(f"[INFO] Fetching: {url}")

    try:
        response = requests.get(
            url,
            headers={"User-Agent": USER_AGENT},
            timeout=REQUEST_TIMEOUT,
            allow_redirects=True,
        )
        response.raise_for_status()
        return response.text

    except requests.exceptions.Timeout:
        print(f"[ERROR] Request timed out after {REQUEST_TIMEOUT} seconds.", file=sys.stderr)
        print(f"[HINT] Check your network connection or try again later.", file=sys.stderr)
        sys.exit(1)

    except requests.exceptions.ConnectionError as e:
        print(f"[ERROR] Connection failed: {e}", file=sys.stderr)
        print(f"[HINT] Verify the URL is correct and the server is reachable.", file=sys.stderr)
        sys.exit(1)

    except requests.exceptions.HTTPError as e:
        print(f"[ERROR] HTTP error: {e}", file=sys.stderr)
        print(f"[HINT] The server returned an error status code.", file=sys.stderr)
        sys.exit(1)

    except requests.exceptions.RequestException as e:
        print(f"[ERROR] Request failed: {e}", file=sys.stderr)
        sys.exit(1)


def find_sidebar_candidates(soup: BeautifulSoup) -> list[Tag]:
    """
    Find all potential sidebar navigation elements using multiple strategies.

    Strategy (in priority order):
        1. nav[aria-label="Navigation"] - ARIA-labeled navigation
        2. nav[role="navigation"] - Role-based navigation
        3. aside nav - Navigation within aside elements
        4. Elements with id/class containing sidebar-related patterns

    Args:
        soup: Parsed BeautifulSoup document.

    Returns:
        List of candidate Tag elements (may be empty).
    """
    candidates: list[Tag] = []
    seen_elements: set[int] = set()  # Track by id() to avoid duplicates

    # Selector priority list
    selectors = [
        'nav[aria-label="Navigation"]',
        "nav[aria-label*='navigation' i]",  # Case-insensitive partial match
        'nav[role="navigation"]',
        "aside nav",
        "nav.md-nav",  # MkDocs Material theme
        "div.md-sidebar nav",  # MkDocs Material sidebar
        ".sidebar nav",
        "#sidebar nav",
        "[class*='sidebar'] nav",
        "[id*='sidebar'] nav",
        "[class*='sidenav']",
        "[id*='sidenav']",
        "nav[class*='nav']",
    ]

    for selector in selectors:
        try:
            elements = soup.select(selector)
            for elem in elements:
                elem_id = id(elem)
                if elem_id not in seen_elements:
                    seen_elements.add(elem_id)
                    candidates.append(elem)
        except Exception:
            # Skip invalid selectors silently
            pass

    # Also look for nav elements generically if nothing found yet
    if not candidates:
        for nav in soup.find_all("nav"):
            elem_id = id(nav)
            if elem_id not in seen_elements:
                seen_elements.add(elem_id)
                candidates.append(nav)

    return candidates


def contains_text(element: Tag, text: str, case_insensitive: bool = True) -> bool:
    """Check if an element contains specific text."""
    element_text = element.get_text(strip=True)
    if case_insensitive:
        return text.lower() in element_text.lower()
    return text in element_text


def count_internal_links(element: Tag, origin: str, base_url: str) -> int:
    """Count the number of internal <a href> links within an element."""
    count = 0
    for anchor in element.find_all("a", href=True):
        href = anchor.get("href", "")
        normalized = normalize_url(href, base_url, origin)
        if normalized:
            count += 1
    return count


def score_sidebar_candidate(
    candidate: Tag, origin: str, base_url: str, verbose: bool = False
) -> tuple[int, str]:
    """
    Score a sidebar candidate element.

    Scoring heuristics:
        - Base score: count of internal links
        - Bonus: +1000 if contains "Navigation" text (case-insensitive)
        - Penalty: -10000 if contains "On this page" (likely TOC)
        - Penalty: -5000 if contains "table of contents" (likely TOC)

    Args:
        candidate: The Tag element to score.
        origin: The origin URL for internal link detection.
        base_url: Base URL for resolving relative hrefs.
        verbose: If True, print scoring details.

    Returns:
        Tuple of (score, reason_string).
    """
    reasons: list[str] = []

    # Count internal links as base score
    link_count = count_internal_links(candidate, origin, base_url)
    score = link_count
    reasons.append(f"internal_links={link_count}")

    # Check for positive indicators
    if contains_text(candidate, "Navigation"):
        score += 1000
        reasons.append("contains 'Navigation' (+1000)")

    # Check for negative indicators (TOC elements)
    if contains_text(candidate, "On this page"):
        score -= 10000
        reasons.append("contains 'On this page' (-10000, likely TOC)")

    if contains_text(candidate, "table of contents"):
        score -= 5000
        reasons.append("contains 'table of contents' (-5000, likely TOC)")

    # Check element attributes for hints
    elem_class = " ".join(candidate.get("class", []))
    elem_id = candidate.get("id", "")
    aria_label = candidate.get("aria-label", "")

    if "sidebar" in elem_class.lower() or "sidebar" in elem_id.lower():
        score += 100
        reasons.append("class/id contains 'sidebar' (+100)")

    if "toc" in elem_class.lower() or "toc" in elem_id.lower():
        score -= 5000
        reasons.append("class/id contains 'toc' (-5000, likely TOC)")

    if "navigation" in aria_label.lower():
        score += 500
        reasons.append("aria-label contains 'navigation' (+500)")

    reason_str = "; ".join(reasons)
    return score, reason_str


def select_best_sidebar(
    candidates: list[Tag], origin: str, base_url: str, verbose: bool = False
) -> Optional[Tag]:
    """
    Select the best sidebar candidate using scoring heuristics.

    Args:
        candidates: List of candidate Tag elements.
        origin: The origin URL for internal link detection.
        base_url: Base URL for resolving relative hrefs.
        verbose: If True, print selection details.

    Returns:
        The best sidebar Tag element, or None if no valid candidate found.
    """
    if not candidates:
        return None

    scored_candidates: list[tuple[int, str, Tag]] = []

    for i, candidate in enumerate(candidates):
        score, reason = score_sidebar_candidate(candidate, origin, base_url, verbose)

        if verbose:
            # Get a brief description of the element
            elem_class = " ".join(candidate.get("class", []))[:50]
            elem_id = candidate.get("id", "")[:30]
            elem_tag = candidate.name
            elem_desc = f"<{elem_tag}"
            if elem_id:
                elem_desc += f' id="{elem_id}"'
            if elem_class:
                elem_desc += f' class="{elem_class}"'
            elem_desc += ">"
            print(f"[DEBUG] Candidate {i + 1}: {elem_desc}")
            print(f"        Score: {score} | Reason: {reason}")

        scored_candidates.append((score, reason, candidate))

    # Sort by score descending
    scored_candidates.sort(key=lambda x: x[0], reverse=True)

    best_score, best_reason, best_candidate = scored_candidates[0]

    # Reject if best score is too low (likely no valid sidebar)
    if best_score < 1:
        return None

    if verbose:
        print(f"[INFO] Selected sidebar with score {best_score}: {best_reason}")

    return best_candidate


def extract_links_from_sidebar(
    sidebar: Tag, origin: str, base_url: str, verbose: bool = False
) -> list[str]:
    """
    Extract and normalize all internal links from the sidebar element.

    Args:
        sidebar: The sidebar Tag element.
        origin: The origin URL for filtering.
        base_url: Base URL for resolving relative hrefs.
        verbose: If True, print extraction details.

    Returns:
        List of unique, normalized URLs in first-seen order.
    """
    seen_urls: set[str] = set()
    unique_links: list[str] = []
    skipped_count = 0
    duplicate_count = 0

    for anchor in sidebar.find_all("a", href=True):
        href = anchor.get("href", "")

        # Normalize the URL
        normalized = normalize_url(href, base_url, origin)

        if normalized is None:
            skipped_count += 1
            continue

        # Deduplicate while preserving order
        if normalized not in seen_urls:
            seen_urls.add(normalized)
            unique_links.append(normalized)
        else:
            duplicate_count += 1

    if verbose:
        print(f"[INFO] Found {len(unique_links)} unique links")
        print(f"[INFO] Skipped {skipped_count} links (external/invalid)")
        print(f"[INFO] Removed {duplicate_count} duplicates")

    return unique_links


def write_links_to_file(links: list[str], output_path: str, verbose: bool = False) -> None:
    """
    Write links to the output file, one per line, in UTF-8.

    Args:
        links: List of URLs to write.
        output_path: Path to the output file.
        verbose: If True, print write confirmation.
    """
    with open(output_path, "w", encoding="utf-8") as f:
        for link in links:
            f.write(link + "\n")

    if verbose:
        print(f"[INFO] Wrote {len(links)} links to {output_path}")


def main() -> None:
    """Main entry point for the CLI tool."""
    parser = argparse.ArgumentParser(
        description="Extract sidebar navigation links from documentation websites.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    python extract_sidebar_links.py
    python extract_sidebar_links.py --start-url https://docs.molt.bot/ --out links.txt
    python extract_sidebar_links.py --verbose
        """,
    )
    parser.add_argument(
        "--start-url",
        default=DEFAULT_START_URL,
        help=f"The starting URL to extract links from (default: {DEFAULT_START_URL})",
    )
    parser.add_argument(
        "--out",
        default=DEFAULT_OUTPUT_FILE,
        help=f"Output file path (default: {DEFAULT_OUTPUT_FILE})",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Print detailed information about sidebar selection and link extraction",
    )

    args = parser.parse_args()

    # Validate start URL
    parsed_start = urlparse(args.start_url)
    if not parsed_start.scheme or not parsed_start.netloc:
        print(f"[ERROR] Invalid URL: {args.start_url}", file=sys.stderr)
        print("[HINT] URL must include scheme (e.g., https://example.com)", file=sys.stderr)
        sys.exit(1)

    origin = get_origin(args.start_url)

    if args.verbose:
        print(f"[INFO] Start URL: {args.start_url}")
        print(f"[INFO] Origin: {origin}")
        print(f"[INFO] Output file: {args.out}")
        print(f"[INFO] Trailing-slash policy: {'STRIP' if STRIP_TRAILING_SLASH else 'KEEP'}")
        print()

    # Fetch the HTML
    html_content = fetch_html(args.start_url, verbose=args.verbose)

    # Parse HTML
    soup = BeautifulSoup(html_content, "html.parser")

    if args.verbose:
        print()
        print("[INFO] Searching for sidebar navigation...")

    # Find sidebar candidates
    candidates = find_sidebar_candidates(soup)

    if args.verbose:
        print(f"[INFO] Found {len(candidates)} potential sidebar candidate(s)")
        print()

    if not candidates:
        print("[ERROR] No sidebar navigation element found.", file=sys.stderr)
        print("[HINT] The page structure may not match expected patterns.", file=sys.stderr)
        print("[HINT] Try inspecting the page HTML to identify the sidebar selector.", file=sys.stderr)
        sys.exit(1)

    # Select the best sidebar
    sidebar = select_best_sidebar(candidates, origin, args.start_url, verbose=args.verbose)

    if sidebar is None:
        print("[ERROR] Could not identify a valid sidebar navigation.", file=sys.stderr)
        print("[HINT] No candidate had a positive score (internal links).", file=sys.stderr)
        sys.exit(1)

    if args.verbose:
        print()

    # Extract links from the selected sidebar
    links = extract_links_from_sidebar(sidebar, origin, args.start_url, verbose=args.verbose)

    if not links:
        print("[WARNING] No internal links found in the sidebar.", file=sys.stderr)
        print("[HINT] The sidebar may be dynamically loaded via JavaScript.", file=sys.stderr)

    # Write to output file
    write_links_to_file(links, args.out, verbose=args.verbose)

    print(f"Successfully extracted {len(links)} unique sidebar links to {args.out}")


if __name__ == "__main__":
    main()
